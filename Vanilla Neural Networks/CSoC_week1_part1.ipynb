{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3ubnWfLFRy1X"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_recall_curve, auc, confusion_matrix\n",
        "import psutil\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "try:\n",
        "        drive.mount('/content/drive')\n",
        "except Exception as e:\n",
        "        print(f\"An error occurred during mounting: {e}\")\n",
        "\n",
        "import zipfile\n",
        "zip_path = '/content/drive/MyDrive/KaggleV2-May-2016.csv.zip'\n",
        "extract_path = '/content/drive/MyDrive/'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EicO-WlBSeH0",
        "outputId": "89c0f306-6b6f-43e1-e500-6184649c878b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_prime(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def softmax(z):\n",
        "    new_z = np.exp(z - np.max(z))\n",
        "    return new_z / np.sum(new_z, axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "class NeuralNetwork(object):\n",
        "\n",
        "    def __init__(self, architecture):\n",
        "\n",
        "        self.L = architecture.size - 1\n",
        "        self.n = architecture\n",
        "\n",
        "        self.parameters = {}\n",
        "\n",
        "        for i in range (1, self.L + 1):\n",
        "            self.parameters['W' + str(i)] = np.random.randn(self.n[i], self.n[i - 1]) * np.sqrt(2. /self.n[i-1])\n",
        "\n",
        "            self.parameters['b' + str(i)] = np.ones((self.n[i], 1))\n",
        "            self.parameters['z' + str(i)] = np.ones((self.n[i], 1))\n",
        "            self.parameters['a' + str(i)] = np.ones((self.n[i], 1))\n",
        "\n",
        "        self.parameters['a0'] = np.ones((self.n[0], 1))\n",
        "\n",
        "        self.parameters['C'] = 1\n",
        "\n",
        "        self.derivatives = {}\n",
        "\n",
        "    def forward_prop(self, X):\n",
        "        self.parameters['a0'] = X\n",
        "\n",
        "        for l in range(1, self.L + 1):\n",
        "            self.parameters['z' + str(l)] = np.dot(self.parameters['W' + str(l)], self.parameters['a' + str(l - 1)]) + self.parameters['b' + str(l)]\n",
        "\n",
        "            if l == self.L:\n",
        "                self.parameters['a' + str(l)] = softmax(self.parameters['z' + str(l)])\n",
        "            else:\n",
        "                self.parameters['a' + str(l)] = relu(self.parameters['z' + str(l)])\n",
        "\n",
        "    def cost(self, y):\n",
        "        aL = self.parameters['a' + str(self.L)]\n",
        "        self.parameters['C'] = -np.sum(y * np.log(aL + 1e-15))\n",
        "\n",
        "    def back_prop(self, y):\n",
        "        self.derivatives['dz' + str(self.L)] = self.parameters['a' + str(self.L)] - y\n",
        "        self.derivatives['dW' + str(self.L)] = np.dot(self.derivatives['dz' + str(self.L)],self.parameters['a' + str(self.L - 1)].T)\n",
        "\n",
        "        self.derivatives['db' + str(self.L)] = self.derivatives['dz' + str(self.L)]\n",
        "\n",
        "        for l in range(self.L - 1, 0, -1):\n",
        "            self.derivatives['dz' + str(l)] = (np.dot(self.parameters['W' + str(l + 1)].T, self.derivatives['dz' + str(l + 1)]) * relu_prime(self.parameters['z' + str(l)]))\n",
        "            self.derivatives['dW' + str(l)] = np.dot(self.derivatives['dz' + str(l)],self.parameters['a' + str(l - 1)].T)\n",
        "            self.derivatives['db' + str(l)] = self.derivatives['dz' + str(l)]\n",
        "\n",
        "    def update_parameters(self, alpha):\n",
        "        for l in range(1, self.L+1):\n",
        "            self.parameters['W' + str(l)] -= alpha*self.derivatives['dW' + str(l)]\n",
        "            self.parameters['b' + str(l)] -= alpha*self.derivatives['db' + str(l)]\n",
        "\n",
        "    def predict(self, x):\n",
        "        self.forward_prop(x)\n",
        "        return self.parameters['a' + str(self.L)]\n",
        "\n",
        "    def fit(self, X, Y, epochs, alpha):\n",
        "        for j in range(0, epochs):\n",
        "            c = 0\n",
        "            n_c = 0\n",
        "\n",
        "            for i in range(0, X.shape[0]):\n",
        "              x = X[i].reshape((X[i].size, 1))\n",
        "              y = Y[i].reshape((-1,1))\n",
        "\n",
        "              self.forward_prop(x)\n",
        "              self.cost(y)\n",
        "              self.back_prop(y)\n",
        "              self.update_parameters(alpha)\n",
        "\n",
        "              c += self.parameters['C']\n",
        "\n",
        "              y_pred = self.predict(x)\n",
        "              if np.argmax(y_pred) == np.argmax(y):\n",
        "                n_c += 1\n",
        "\n",
        "\n",
        "            c = c/X.shape[0]\n",
        "            print('Iteration: ', j)\n",
        "            print(\"Cost :\",c)\n",
        "            print(\"Accuracy : \",(n_c/X.shape[0])*100)"
      ],
      "metadata": {
        "id": "qbD4gPy6R_bm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/KaggleV2-May-2016.csv')\n",
        "\n",
        "df=df.drop(columns=['PatientId','AppointmentID','ScheduledDay','AppointmentDay',])\n",
        "df.rename(columns={'No-show':'given'},inplace = True)\n",
        "df['given'] = df['given'].map({'No':0,'Yes':1})\n",
        "df['Gender'] = df['Gender'].map({'F':0,'M':1})\n",
        "\n",
        "df = pd.get_dummies(df, columns=[\"Neighbourhood\"])\n",
        "\n",
        "features = ['Gender', 'Age', 'Scholarship', 'Hipertension',\n",
        "            'Diabetes', 'Alcoholism', 'Handcap', 'SMS_received'] + [col for col in df.columns if col.startswith(\"Neighbourhood_\")]\n",
        "\n",
        "X = df[features].values\n",
        "y = df['given'].values\n",
        "\n",
        "SS_X = StandardScaler()\n",
        "X = SS_X.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3)\n",
        "y_train_encoded = np.zeros((y_train.size, 2))\n",
        "y_train_encoded[np.arange(y_train.size), y_train] = 1\n",
        "\n",
        "input_size = X_train.shape[1]  # actual number of input features\n",
        "architecture = np.array([input_size, 64, 32, 2])\n",
        "classifier = NeuralNetwork(architecture)\n",
        "\n",
        "classifier.fit(X_train,y_train_encoded,200,1e-4)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB1H5aZuSJpt",
        "outputId": "505c775b-343d-4f29-ce81-127c2c57d1d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0\n",
            "Cost : 0.5267908727478853\n",
            "Accuracy :  79.12832178678524\n",
            "Iteration:  1\n",
            "Cost : 0.49803903721949144\n",
            "Accuracy :  79.68281460035158\n",
            "Iteration:  2\n",
            "Cost : 0.49618013285774765\n",
            "Accuracy :  79.68539964843346\n",
            "Iteration:  3\n",
            "Cost : 0.4952571775574647\n",
            "Accuracy :  79.68927722055629\n",
            "Iteration:  4\n",
            "Cost : 0.4947035176145459\n",
            "Accuracy :  79.68669217247441\n",
            "Iteration:  5\n",
            "Cost : 0.49429998767130784\n",
            "Accuracy :  79.68669217247441\n",
            "Iteration:  6\n",
            "Cost : 0.4939839862622645\n",
            "Accuracy :  79.68669217247441\n",
            "Iteration:  7\n",
            "Cost : 0.49371342140637897\n",
            "Accuracy :  79.68669217247441\n",
            "Iteration:  8\n",
            "Cost : 0.49347303818234955\n",
            "Accuracy :  79.68669217247441\n",
            "Iteration:  9\n",
            "Cost : 0.49325885462866825\n",
            "Accuracy :  79.68669217247441\n",
            "Iteration:  10\n",
            "Cost : 0.49306174302116984\n",
            "Accuracy :  79.68798469651536\n",
            "Iteration:  11\n",
            "Cost : 0.4928883958968034\n",
            "Accuracy :  79.68798469651536\n",
            "Iteration:  12\n",
            "Cost : 0.4927300055888875\n",
            "Accuracy :  79.68798469651536\n",
            "Iteration:  13\n",
            "Cost : 0.492577166204943\n",
            "Accuracy :  79.68798469651536\n",
            "Iteration:  14\n",
            "Cost : 0.49243625019469206\n",
            "Accuracy :  79.68798469651536\n",
            "Iteration:  15\n",
            "Cost : 0.4923039258982813\n",
            "Accuracy :  79.68798469651536\n",
            "Iteration:  16\n",
            "Cost : 0.4921825152979738\n",
            "Accuracy :  79.68669217247441\n",
            "Iteration:  17\n",
            "Cost : 0.4920634030562394\n",
            "Accuracy :  79.68669217247441\n",
            "Iteration:  18\n",
            "Cost : 0.4919448647800749\n",
            "Accuracy :  79.68669217247441\n",
            "Iteration:  19\n",
            "Cost : 0.4918361862582254\n",
            "Accuracy :  79.68669217247441\n",
            "Iteration:  20\n",
            "Cost : 0.4917313331969495\n",
            "Accuracy :  79.68669217247441\n",
            "Iteration:  21\n",
            "Cost : 0.49163292332481084\n",
            "Accuracy :  79.68798469651536\n",
            "Iteration:  22\n",
            "Cost : 0.4915384780732605\n",
            "Accuracy :  79.68798469651536\n",
            "Iteration:  23\n",
            "Cost : 0.4914449924984049\n",
            "Accuracy :  79.68798469651536\n",
            "Iteration:  24\n",
            "Cost : 0.4913549062236373\n",
            "Accuracy :  79.68927722055629\n",
            "Iteration:  25\n",
            "Cost : 0.49126149674914765\n",
            "Accuracy :  79.68927722055629\n",
            "Iteration:  26\n",
            "Cost : 0.4911741947962147\n",
            "Accuracy :  79.68927722055629\n",
            "Iteration:  27\n",
            "Cost : 0.4910854780413489\n",
            "Accuracy :  79.68927722055629\n",
            "Iteration:  28\n",
            "Cost : 0.4909970291098145\n",
            "Accuracy :  79.68927722055629\n",
            "Iteration:  29\n",
            "Cost : 0.4909073008681899\n",
            "Accuracy :  79.68927722055629\n",
            "Iteration:  30\n",
            "Cost : 0.4908238433656418\n",
            "Accuracy :  79.68927722055629\n",
            "Iteration:  31\n",
            "Cost : 0.49074339441620535\n",
            "Accuracy :  79.69056974459725\n",
            "Iteration:  32\n",
            "Cost : 0.49066083178010406\n",
            "Accuracy :  79.69056974459725\n",
            "Iteration:  33\n",
            "Cost : 0.49057696480294627\n",
            "Accuracy :  79.68927722055629\n",
            "Iteration:  34\n",
            "Cost : 0.49049384525929607\n",
            "Accuracy :  79.68798469651536\n",
            "Iteration:  35\n",
            "Cost : 0.4904179431524969\n",
            "Accuracy :  79.68798469651536\n",
            "Iteration:  36\n",
            "Cost : 0.4903409576012583\n",
            "Accuracy :  79.68927722055629\n",
            "Iteration:  37\n",
            "Cost : 0.49026971198280106\n",
            "Accuracy :  79.68927722055629\n",
            "Iteration:  38\n",
            "Cost : 0.49019622282962066\n",
            "Accuracy :  79.68927722055629\n",
            "Iteration:  39\n",
            "Cost : 0.4901266503452748\n",
            "Accuracy :  79.68927722055629\n",
            "Iteration:  40\n",
            "Cost : 0.49004966933241706\n",
            "Accuracy :  79.68927722055629\n",
            "Iteration:  41\n",
            "Cost : 0.48997510041397885\n",
            "Accuracy :  79.68927722055629\n",
            "Iteration:  42\n",
            "Cost : 0.4899039049161744\n",
            "Accuracy :  79.68927722055629\n",
            "Iteration:  43\n",
            "Cost : 0.48983462244507353\n",
            "Accuracy :  79.69186226863819\n",
            "Iteration:  44\n",
            "Cost : 0.4897603275205761\n",
            "Accuracy :  79.69186226863819\n",
            "Iteration:  45\n",
            "Cost : 0.4896914666320382\n",
            "Accuracy :  79.69186226863819\n",
            "Iteration:  46\n",
            "Cost : 0.48961760671748955\n",
            "Accuracy :  79.69056974459725\n",
            "Iteration:  47\n",
            "Cost : 0.48955389061479065\n",
            "Accuracy :  79.69186226863819\n",
            "Iteration:  48\n",
            "Cost : 0.48948622460355007\n",
            "Accuracy :  79.69186226863819\n",
            "Iteration:  49\n",
            "Cost : 0.489425916697693\n",
            "Accuracy :  79.69186226863819\n",
            "Iteration:  50\n",
            "Cost : 0.4893671920543636\n",
            "Accuracy :  79.69186226863819\n",
            "Iteration:  51\n",
            "Cost : 0.4893050808293339\n",
            "Accuracy :  79.69315479267914\n",
            "Iteration:  52\n",
            "Cost : 0.48924395437896645\n",
            "Accuracy :  79.69573984076104\n",
            "Iteration:  53\n",
            "Cost : 0.4891871180102293\n",
            "Accuracy :  79.69832488884293\n",
            "Iteration:  54\n",
            "Cost : 0.48913056785451736\n",
            "Accuracy :  79.69832488884293\n",
            "Iteration:  55\n",
            "Cost : 0.4890710016123825\n",
            "Accuracy :  79.69832488884293\n",
            "Iteration:  56\n",
            "Cost : 0.48901224625070117\n",
            "Accuracy :  79.69832488884293\n",
            "Iteration:  57\n",
            "Cost : 0.4889550487918762\n",
            "Accuracy :  79.69573984076104\n",
            "Iteration:  58\n",
            "Cost : 0.4888948304924439\n",
            "Accuracy :  79.69573984076104\n",
            "Iteration:  59\n",
            "Cost : 0.48883354625005015\n",
            "Accuracy :  79.69703236480198\n",
            "Iteration:  60\n",
            "Cost : 0.4887779296559643\n",
            "Accuracy :  79.69703236480198\n",
            "Iteration:  61\n",
            "Cost : 0.48871626656243977\n",
            "Accuracy :  79.69832488884293\n",
            "Iteration:  62\n",
            "Cost : 0.48866091212882395\n",
            "Accuracy :  79.69832488884293\n",
            "Iteration:  63\n",
            "Cost : 0.48860297078004655\n",
            "Accuracy :  79.69832488884293\n",
            "Iteration:  64\n",
            "Cost : 0.48854535291665463\n",
            "Accuracy :  79.69961741288388\n",
            "Iteration:  65\n",
            "Cost : 0.488489305782486\n",
            "Accuracy :  79.69832488884293\n",
            "Iteration:  66\n",
            "Cost : 0.48843627134310236\n",
            "Accuracy :  79.70349498500671\n",
            "Iteration:  67\n",
            "Cost : 0.48837942534713064\n",
            "Accuracy :  79.70478750904768\n",
            "Iteration:  68\n",
            "Cost : 0.4883248063673514\n",
            "Accuracy :  79.70349498500671\n",
            "Iteration:  69\n",
            "Cost : 0.48827853387717535\n",
            "Accuracy :  79.70349498500671\n",
            "Iteration:  70\n",
            "Cost : 0.4882316053910528\n",
            "Accuracy :  79.70478750904768\n",
            "Iteration:  71\n",
            "Cost : 0.4881840105465279\n",
            "Accuracy :  79.70478750904768\n",
            "Iteration:  72\n",
            "Cost : 0.4881289277429139\n",
            "Accuracy :  79.70608003308861\n",
            "Iteration:  73\n",
            "Cost : 0.4880748699248179\n",
            "Accuracy :  79.70478750904768\n",
            "Iteration:  74\n",
            "Cost : 0.488026422639167\n",
            "Accuracy :  79.70478750904768\n",
            "Iteration:  75\n",
            "Cost : 0.4879735842786461\n",
            "Accuracy :  79.70737255712956\n",
            "Iteration:  76\n",
            "Cost : 0.4879294233729927\n",
            "Accuracy :  79.70995760521146\n",
            "Iteration:  77\n",
            "Cost : 0.4878792680985924\n",
            "Accuracy :  79.70995760521146\n",
            "Iteration:  78\n",
            "Cost : 0.48782534629318103\n",
            "Accuracy :  79.70995760521146\n",
            "Iteration:  79\n",
            "Cost : 0.48777563755162595\n",
            "Accuracy :  79.70737255712956\n",
            "Iteration:  80\n",
            "Cost : 0.48773001110617037\n",
            "Accuracy :  79.70737255712956\n",
            "Iteration:  81\n",
            "Cost : 0.4876813691796201\n",
            "Accuracy :  79.70995760521146\n",
            "Iteration:  82\n",
            "Cost : 0.4876316663738976\n",
            "Accuracy :  79.70995760521146\n",
            "Iteration:  83\n",
            "Cost : 0.48759016252689424\n",
            "Accuracy :  79.7112501292524\n",
            "Iteration:  84\n",
            "Cost : 0.4875437512282623\n",
            "Accuracy :  79.7112501292524\n",
            "Iteration:  85\n",
            "Cost : 0.48750363314562717\n",
            "Accuracy :  79.7112501292524\n",
            "Iteration:  86\n",
            "Cost : 0.4874535762701553\n",
            "Accuracy :  79.7138351773343\n",
            "Iteration:  87\n",
            "Cost : 0.4874107623282649\n",
            "Accuracy :  79.71512770137524\n",
            "Iteration:  88\n",
            "Cost : 0.4873641611588882\n",
            "Accuracy :  79.7138351773343\n",
            "Iteration:  89\n",
            "Cost : 0.4873150673527119\n",
            "Accuracy :  79.71512770137524\n",
            "Iteration:  90\n",
            "Cost : 0.4872687847115321\n",
            "Accuracy :  79.7138351773343\n",
            "Iteration:  91\n",
            "Cost : 0.48721944243815163\n",
            "Accuracy :  79.71512770137524\n",
            "Iteration:  92\n",
            "Cost : 0.48717474280655176\n",
            "Accuracy :  79.71771274945714\n",
            "Iteration:  93\n",
            "Cost : 0.4871282125719461\n",
            "Accuracy :  79.72159032157998\n",
            "Iteration:  94\n",
            "Cost : 0.48708226149782985\n",
            "Accuracy :  79.72159032157998\n",
            "Iteration:  95\n",
            "Cost : 0.48703831519279217\n",
            "Accuracy :  79.72546789370283\n",
            "Iteration:  96\n",
            "Cost : 0.48698924390701037\n",
            "Accuracy :  79.72417536966188\n",
            "Iteration:  97\n",
            "Cost : 0.48694441191366583\n",
            "Accuracy :  79.72288284562093\n",
            "Iteration:  98\n",
            "Cost : 0.48689641303784226\n",
            "Accuracy :  79.72676041774378\n",
            "Iteration:  99\n",
            "Cost : 0.48685247833172063\n",
            "Accuracy :  79.72546789370283\n",
            "Iteration:  100\n",
            "Cost : 0.48680659169574986\n",
            "Accuracy :  79.72546789370283\n",
            "Iteration:  101\n",
            "Cost : 0.48676257496297576\n",
            "Accuracy :  79.72805294178472\n",
            "Iteration:  102\n",
            "Cost : 0.48672020924785503\n",
            "Accuracy :  79.73063798986661\n",
            "Iteration:  103\n",
            "Cost : 0.48667279216871306\n",
            "Accuracy :  79.73193051390756\n",
            "Iteration:  104\n",
            "Cost : 0.4866295867781909\n",
            "Accuracy :  79.7332230379485\n",
            "Iteration:  105\n",
            "Cost : 0.48658726938207775\n",
            "Accuracy :  79.73451556198945\n",
            "Iteration:  106\n",
            "Cost : 0.48654215918904015\n",
            "Accuracy :  79.7358080860304\n",
            "Iteration:  107\n",
            "Cost : 0.48649720074916664\n",
            "Accuracy :  79.7358080860304\n",
            "Iteration:  108\n",
            "Cost : 0.48645057908690315\n",
            "Accuracy :  79.7358080860304\n",
            "Iteration:  109\n",
            "Cost : 0.4864057200091172\n",
            "Accuracy :  79.74227070623515\n",
            "Iteration:  110\n",
            "Cost : 0.4863553917052942\n",
            "Accuracy :  79.74227070623515\n",
            "Iteration:  111\n",
            "Cost : 0.4863099092476828\n",
            "Accuracy :  79.74744080239893\n",
            "Iteration:  112\n",
            "Cost : 0.486270704615386\n",
            "Accuracy :  79.74873332643988\n",
            "Iteration:  113\n",
            "Cost : 0.48622689844039607\n",
            "Accuracy :  79.75002585048082\n",
            "Iteration:  114\n",
            "Cost : 0.48619097275606177\n",
            "Accuracy :  79.74873332643988\n",
            "Iteration:  115\n",
            "Cost : 0.48615335925898856\n",
            "Accuracy :  79.75131837452176\n",
            "Iteration:  116\n",
            "Cost : 0.48611480852279065\n",
            "Accuracy :  79.75648847068555\n",
            "Iteration:  117\n",
            "Cost : 0.4860778726023019\n",
            "Accuracy :  79.75648847068555\n",
            "Iteration:  118\n",
            "Cost : 0.48603623231263804\n",
            "Accuracy :  79.75648847068555\n",
            "Iteration:  119\n",
            "Cost : 0.48600081725022076\n",
            "Accuracy :  79.7577809947265\n",
            "Iteration:  120\n",
            "Cost : 0.4859684169254911\n",
            "Accuracy :  79.75907351876745\n",
            "Iteration:  121\n",
            "Cost : 0.4859305197048954\n",
            "Accuracy :  79.75648847068555\n",
            "Iteration:  122\n",
            "Cost : 0.4858993547642861\n",
            "Accuracy :  79.75907351876745\n",
            "Iteration:  123\n",
            "Cost : 0.48586131086417855\n",
            "Accuracy :  79.7603660428084\n",
            "Iteration:  124\n",
            "Cost : 0.4858271828249531\n",
            "Accuracy :  79.7603660428084\n",
            "Iteration:  125\n",
            "Cost : 0.48578729081024646\n",
            "Accuracy :  79.7577809947265\n",
            "Iteration:  126\n",
            "Cost : 0.485760866710609\n",
            "Accuracy :  79.7603660428084\n",
            "Iteration:  127\n",
            "Cost : 0.48571897273709796\n",
            "Accuracy :  79.7603660428084\n",
            "Iteration:  128\n",
            "Cost : 0.48568313702446086\n",
            "Accuracy :  79.7603660428084\n",
            "Iteration:  129\n",
            "Cost : 0.4856506484141664\n",
            "Accuracy :  79.76553613897218\n",
            "Iteration:  130\n",
            "Cost : 0.4856136632530042\n",
            "Accuracy :  79.76424361493125\n",
            "Iteration:  131\n",
            "Cost : 0.4855792891639703\n",
            "Accuracy :  79.76682866301313\n",
            "Iteration:  132\n",
            "Cost : 0.48554237584192317\n",
            "Accuracy :  79.77070623513598\n",
            "Iteration:  133\n",
            "Cost : 0.48550017117366984\n",
            "Accuracy :  79.76682866301313\n",
            "Iteration:  134\n",
            "Cost : 0.4854620025080283\n",
            "Accuracy :  79.76941371109503\n",
            "Iteration:  135\n",
            "Cost : 0.48543026644434595\n",
            "Accuracy :  79.77070623513598\n",
            "Iteration:  136\n",
            "Cost : 0.4853978733797418\n",
            "Accuracy :  79.77070623513598\n",
            "Iteration:  137\n",
            "Cost : 0.48536335760956345\n",
            "Accuracy :  79.77199875917692\n",
            "Iteration:  138\n",
            "Cost : 0.4853270736864342\n",
            "Accuracy :  79.77070623513598\n",
            "Iteration:  139\n",
            "Cost : 0.4852916448896534\n",
            "Accuracy :  79.77070623513598\n",
            "Iteration:  140\n",
            "Cost : 0.4852540108363136\n",
            "Accuracy :  79.76812118705408\n",
            "Iteration:  141\n",
            "Cost : 0.4852085399826329\n",
            "Accuracy :  79.76682866301313\n",
            "Iteration:  142\n",
            "Cost : 0.48518154076138204\n",
            "Accuracy :  79.76424361493125\n",
            "Iteration:  143\n",
            "Cost : 0.4851507671003381\n",
            "Accuracy :  79.76553613897218\n",
            "Iteration:  144\n",
            "Cost : 0.4851111534792949\n",
            "Accuracy :  79.76553613897218\n",
            "Iteration:  145\n",
            "Cost : 0.4850827084465071\n",
            "Accuracy :  79.76553613897218\n",
            "Iteration:  146\n",
            "Cost : 0.48504578770822976\n",
            "Accuracy :  79.76553613897218\n",
            "Iteration:  147\n",
            "Cost : 0.48501774472377057\n",
            "Accuracy :  79.76165856684935\n",
            "Iteration:  148\n",
            "Cost : 0.48498880324772653\n",
            "Accuracy :  79.7603660428084\n",
            "Iteration:  149\n",
            "Cost : 0.48495635728592507\n",
            "Accuracy :  79.7603660428084\n",
            "Iteration:  150\n",
            "Cost : 0.48492858011694057\n",
            "Accuracy :  79.76165856684935\n",
            "Iteration:  151\n",
            "Cost : 0.48489590149232487\n",
            "Accuracy :  79.76553613897218\n",
            "Iteration:  152\n",
            "Cost : 0.48486935910348394\n",
            "Accuracy :  79.76812118705408\n",
            "Iteration:  153\n",
            "Cost : 0.4848374862726479\n",
            "Accuracy :  79.76941371109503\n",
            "Iteration:  154\n",
            "Cost : 0.48480798280670734\n",
            "Accuracy :  79.77070623513598\n",
            "Iteration:  155\n",
            "Cost : 0.48478014866860886\n",
            "Accuracy :  79.76941371109503\n",
            "Iteration:  156\n",
            "Cost : 0.4847438329522275\n",
            "Accuracy :  79.77329128321787\n",
            "Iteration:  157\n",
            "Cost : 0.48470800612368303\n",
            "Accuracy :  79.77587633129977\n",
            "Iteration:  158\n",
            "Cost : 0.48467644616136496\n",
            "Accuracy :  79.7797539034226\n",
            "Iteration:  159\n",
            "Cost : 0.48464640666483616\n",
            "Accuracy :  79.7797539034226\n",
            "Iteration:  160\n",
            "Cost : 0.4846183990543693\n",
            "Accuracy :  79.77846137938165\n",
            "Iteration:  161\n",
            "Cost : 0.4845985609554225\n",
            "Accuracy :  79.77458380725881\n",
            "Iteration:  162\n",
            "Cost : 0.48455890209692004\n",
            "Accuracy :  79.7771688553407\n",
            "Iteration:  163\n",
            "Cost : 0.4845341575067842\n",
            "Accuracy :  79.7771688553407\n",
            "Iteration:  164\n",
            "Cost : 0.4845050822209712\n",
            "Accuracy :  79.7771688553407\n",
            "Iteration:  165\n",
            "Cost : 0.4844743508174888\n",
            "Accuracy :  79.7771688553407\n",
            "Iteration:  166\n",
            "Cost : 0.4844453361556311\n",
            "Accuracy :  79.7771688553407\n",
            "Iteration:  167\n",
            "Cost : 0.4844186376751026\n",
            "Accuracy :  79.7797539034226\n",
            "Iteration:  168\n",
            "Cost : 0.48439380719556296\n",
            "Accuracy :  79.7797539034226\n",
            "Iteration:  169\n",
            "Cost : 0.4843620974326922\n",
            "Accuracy :  79.7771688553407\n",
            "Iteration:  170\n",
            "Cost : 0.4843329360832352\n",
            "Accuracy :  79.77587633129977\n",
            "Iteration:  171\n",
            "Cost : 0.48430762181115417\n",
            "Accuracy :  79.7771688553407\n",
            "Iteration:  172\n",
            "Cost : 0.48428198728829813\n",
            "Accuracy :  79.77587633129977\n",
            "Iteration:  173\n",
            "Cost : 0.4842504145569283\n",
            "Accuracy :  79.77846137938165\n",
            "Iteration:  174\n",
            "Cost : 0.48422947730148785\n",
            "Accuracy :  79.77846137938165\n",
            "Iteration:  175\n",
            "Cost : 0.48419927094762144\n",
            "Accuracy :  79.7823389515045\n",
            "Iteration:  176\n",
            "Cost : 0.48417079349189523\n",
            "Accuracy :  79.78492399958638\n",
            "Iteration:  177\n",
            "Cost : 0.4841437690880933\n",
            "Accuracy :  79.78104642746355\n",
            "Iteration:  178\n",
            "Cost : 0.48411444072773946\n",
            "Accuracy :  79.78363147554545\n",
            "Iteration:  179\n",
            "Cost : 0.48408761034232284\n",
            "Accuracy :  79.78492399958638\n",
            "Iteration:  180\n",
            "Cost : 0.4840547234773607\n",
            "Accuracy :  79.78750904766828\n",
            "Iteration:  181\n",
            "Cost : 0.48402753378768726\n",
            "Accuracy :  79.78621652362735\n",
            "Iteration:  182\n",
            "Cost : 0.48399964847897137\n",
            "Accuracy :  79.78880157170923\n",
            "Iteration:  183\n",
            "Cost : 0.4839633653367875\n",
            "Accuracy :  79.79009409575019\n",
            "Iteration:  184\n",
            "Cost : 0.4839333978600267\n",
            "Accuracy :  79.79009409575019\n",
            "Iteration:  185\n",
            "Cost : 0.48390745923797385\n",
            "Accuracy :  79.79267914383207\n",
            "Iteration:  186\n",
            "Cost : 0.4838763321663184\n",
            "Accuracy :  79.79526419191397\n",
            "Iteration:  187\n",
            "Cost : 0.4838484638169788\n",
            "Accuracy :  79.79655671595492\n",
            "Iteration:  188\n",
            "Cost : 0.4838180634225471\n",
            "Accuracy :  79.79655671595492\n",
            "Iteration:  189\n",
            "Cost : 0.4837897897748889\n",
            "Accuracy :  79.7991417640368\n",
            "Iteration:  190\n",
            "Cost : 0.4837697984010605\n",
            "Accuracy :  79.7991417640368\n",
            "Iteration:  191\n",
            "Cost : 0.4837449062713194\n",
            "Accuracy :  79.79784923999587\n",
            "Iteration:  192\n",
            "Cost : 0.4837204083777132\n",
            "Accuracy :  79.80301933615965\n",
            "Iteration:  193\n",
            "Cost : 0.48369162554033207\n",
            "Accuracy :  79.8043118602006\n",
            "Iteration:  194\n",
            "Cost : 0.4836644544934051\n",
            "Accuracy :  79.80043428807775\n",
            "Iteration:  195\n",
            "Cost : 0.4836449943839836\n",
            "Accuracy :  79.80043428807775\n",
            "Iteration:  196\n",
            "Cost : 0.4836242124051213\n",
            "Accuracy :  79.8017268121187\n",
            "Iteration:  197\n",
            "Cost : 0.4835948128041053\n",
            "Accuracy :  79.80043428807775\n",
            "Iteration:  198\n",
            "Cost : 0.4835678183350674\n",
            "Accuracy :  79.8043118602006\n",
            "Iteration:  199\n",
            "Cost : 0.48354528750065007\n",
            "Accuracy :  79.80560438424155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = []\n",
        "y_probs = []\n",
        "\n",
        "for i in range(X_test.shape[0]):\n",
        "    x = X_test[i].reshape((-1, 1))\n",
        "    prob = classifier.predict(x)\n",
        "    y_probs.append(prob.ravel())\n",
        "    y_preds.append(np.argmax(prob))\n",
        "\n",
        "y_probs = np.array(y_probs)\n",
        "y_preds = np.array(y_preds)\n",
        "\n",
        "# Accuracy\n",
        "acc = accuracy_score(y_test, y_preds)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, y_preds)\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_probs[:, 1])\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f\"PR AUC: {pr_auc:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_preds)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "def get_memory_usage():\n",
        "    process = psutil.Process()\n",
        "    mem_info = process.memory_info()\n",
        "    memory_used_mb = mem_info.rss / 1024 ** 2\n",
        "    print(\"Memory Usage (MB):\")\n",
        "    print(memory_used_mb)\n",
        "get_memory_usage()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RABcF9IpuEct",
        "outputId": "69d54ec0-0da3-46c9-f048-9e38ba008f60"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8010\n",
            "F1 Score: 0.0140\n",
            "PR AUC: 0.2759\n",
            "Confusion Matrix:\n",
            "[[26512    50]\n",
            " [ 6550    47]]\n",
            "Memory Usage (MB):\n",
            "451.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJsAu6obuPxy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}